{"cells":[{"cell_type":"code","execution_count":null,"id":"243cb3f5-3a4c-4940-9aeb-2a123349e0b4","metadata":{"colab":{"background_save":true},"id":"243cb3f5-3a4c-4940-9aeb-2a123349e0b4"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.applications import MobileNetV2, VGG16, MobileNetV3Small, EfficientNetB0, EfficientNetV2B0, ResNet50\n","from tensorflow.keras.layers import AveragePooling2D, Dropout, Flatten, Dense, Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from skimage import exposure\n","import os\n","from tqdm import tqdm\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import glob as gb\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"id":"QhjrG1LKh87i","metadata":{"id":"QhjrG1LKh87i"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"2de01008-8bed-417e-ae2b-e653ea808e3b","metadata":{"id":"2de01008-8bed-417e-ae2b-e653ea808e3b"},"outputs":[],"source":["# Constants and Hyperparameters\n","resize = 224\n","learning_rate = 1e-4\n","seed = 107\n","INIT_LR = 1e-4\n","EPOCHS = 3     #set to 30 at orginal presentation\n","BS = 64\n","\n","# Directory paths\n","base_dir = '/content/drive/MyDrive/Project/Data/skin_dataset'\n","TRAIN_DIR = os.path.join(base_dir, 'train')\n","TEST_DIR = os.path.join(base_dir, 'test')\n","VALID_DIR = os.path.join(base_dir, 'valid')\n","\n","# Check if directories exist\n","assert os.path.exists(TRAIN_DIR), 'Could not find' + TRAIN_DIR\n","assert os.path.exists(TEST_DIR), 'Could not find' + TEST_DIR"]},{"cell_type":"code","execution_count":null,"id":"3b8dafa4-a547-4c0e-820a-257d008c4252","metadata":{"id":"3b8dafa4-a547-4c0e-820a-257d008c4252"},"outputs":[],"source":["# Data visualization\n","categories = []\n","class_count = []\n","train_exm = 0\n","\n","for f in tqdm(os.listdir(TRAIN_DIR)):\n","    files = gb.glob(pathname=str(TRAIN_DIR + '/' + f + '/*'))\n","    categories.append(f)\n","    class_count.append(len(files))\n","    train_exm += len(files)\n","\n","# sns.barplot(x=categories, y=class_count).set_title(\"Distribution of Train Data\")\n","# plt.show()\n","\n","# Using seaborn for barplot\n","sns.set_style('whitegrid')\n","plt.figure(figsize=(10, 6))\n","ax = sns.barplot(x=categories, y=class_count, hue=categories, palette='flare', dodge=False)\n","\n","# Add labels to the bars\n","for container in ax.containers:\n","    ax.bar_label(container)\n","\n","plt.title('Distribution of Train Data')\n","plt.xticks(rotation=45)\n","\n","# Check if legend exists before trying to remove it\n","if ax.legend_ is not None:\n","    ax.legend_.remove()\n","\n","plt.show()\n","print(train_exm)\n","\n","\n","# Data loading and preprocessing\n","data = []\n","labels = []\n","\n","for c in categories:\n","    path = os.path.join(TRAIN_DIR, c)\n","    for img in tqdm(os.listdir(path)):\n","        img_path = os.path.join(path, img)\n","        image = load_img(img_path, target_size=(resize, resize))\n","        image = img_to_array(image)\n","        image = preprocess_input(image)\n","\n","        data.append(image)\n","        labels.append(c)\n","\n","data = np.array(data, dtype=\"float32\")\n","labels = np.array(labels)"]},{"cell_type":"code","execution_count":null,"id":"b0b33224-4cfd-40da-83e3-31f27bcba61e","metadata":{"id":"b0b33224-4cfd-40da-83e3-31f27bcba61e"},"outputs":[],"source":["len(labels)"]},{"cell_type":"code","execution_count":null,"id":"1fdfbfa4-9aee-47d0-aa36-41e0b04834ab","metadata":{"id":"1fdfbfa4-9aee-47d0-aa36-41e0b04834ab"},"outputs":[],"source":["unique, counts = np.unique(labels, return_counts=True)\n","dict(zip(unique, counts))"]},{"cell_type":"code","execution_count":null,"id":"14513b2e-9d57-4cbe-9db5-7528683a8819","metadata":{"id":"14513b2e-9d57-4cbe-9db5-7528683a8819"},"outputs":[],"source":["# Encode the labels in one hot encode form\n","lb = LabelEncoder()\n","labels = lb.fit_transform(labels)\n","labels = to_categorical(labels)"]},{"cell_type":"code","execution_count":null,"id":"1c2205d7-65a6-497f-a7c3-f3fbe79b479a","metadata":{"id":"1c2205d7-65a6-497f-a7c3-f3fbe79b479a"},"outputs":[],"source":["# Data Augmentation\n","AugmentedData = ImageDataGenerator(\n","    zoom_range=0.15, #Randomly zooms the images zoom in up to 15%\n","    rotation_range=360,  # Randomly rotates the images Set to 360 to cover all possible rotations\n","    width_shift_range=0.1, #Randomly shifts the images horizontally\n","    height_shift_range=0.1, #Randomly shifts the images vertically\n","    #shear transformation will be applied with a maximum shear angle of 0.15 degreesto 0.20. shear_range=(0.15, 0.2)\n","    shear_range=0.15, #A positive shear angle tilts the image to the right, while a negative shear angle tilts it to the left.\n","    #handle variations in facial structure\n","    brightness_range=[0.5, 1.5], #brightness of the images can be adjusted to be as dark as 50% of the original brightness or as bright as 150%\n","    # rescale=1./255,  #rescaling for data preprocessing pixel values in the range [0, 255], dividing by 255 ensures that the values are scaled to be between 0 and 1 NORMALIZATION\n","    horizontal_flip=True, #Randomly flips the images horizontally\n","    fill_mode=\"nearest\" #Strategy for filling in newly created pixels\n","    #Options include \"nearest\" (fill with nearest pixel value) or others like \"constant,\" \"reflect,\" or \"wrap.\"\n",")\n","\n","# Custom function for contrast stretching\n","def contrast_stretching(image):\n","    p2, p98 = np.percentile(image, (2, 98)) #2nd and 98th percentiles of pixel intensities in the image.\n","    return exposure.rescale_intensity(image, in_range=(p2, p98)) #skimage library \u0026 exposure\n","\n","# Applying contrast stretching during data augmentation\n","AugmentedData.preprocessing_function = contrast_stretching\n","\n","# contrast_stretching=(0.5, 2.0), #contrast of the images can be adjusted to be as low as 50% of the original contrast or as high as 100%"]},{"cell_type":"code","execution_count":null,"id":"838fabf6-9bb1-4fc6-aee8-ccaaab5ce05f","metadata":{"id":"838fabf6-9bb1-4fc6-aee8-ccaaab5ce05f"},"outputs":[],"source":["# Model names dictionary\n","model_dict = {\n","    'VGG16': VGG16,\n","    'MobileNetV2' : MobileNetV2,\n","    'MobileNetV3Small': MobileNetV3Small,\n","    'ResNet50': ResNet50,\n","    'EfficientNetB0': EfficientNetB0,\n","    'EfficientNetV2B0': EfficientNetV2B0\n","}"]},{"cell_type":"code","execution_count":null,"id":"563dc703-84c5-456f-983d-6c716aee2a9f","metadata":{"id":"563dc703-84c5-456f-983d-6c716aee2a9f"},"outputs":[],"source":["# Choose the desired model\n","model_name = 'VGG16'\n","base_model = model_dict[model_name](weights='imagenet', include_top=False, input_shape=(resize, resize, 3))\n","base_model.summary()\n","\n","# (A simple CNN as the Head model) a custom head for classification\n","head_model = base_model.output\n","head_model = AveragePooling2D(pool_size=(7, 7))(head_model)\n","head_model = Flatten(name=\"flatten\")(head_model)\n","head_model = Dense(256, activation=\"relu\")(head_model)\n","head_model = Dropout(0.25)(head_model)\n","head_model = Dense(3, activation=\"softmax\")(head_model)\n","\n","# Combine the base model and head model\n","model = Model(inputs=base_model.input, outputs=head_model)\n","\n","# Freeze the layers of the base model\n","for layer in base_model.layers:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":null,"id":"0c377ffd-ba7a-41f5-800d-559b80fcc1a9","metadata":{"id":"0c377ffd-ba7a-41f5-800d-559b80fcc1a9"},"outputs":[],"source":["# Split data into training and testing sets\n","(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2, stratify=labels, random_state=42)\n","print(f\"Shape of x_train: {trainX.shape}\")\n","print(f\"Shape of y_train: {trainY.shape}\")\n","print()\n","print(f\"Shape of x_test: {testX.shape}\")\n","print(f\"Shape of y_test: {testY.shape}\")"]},{"cell_type":"code","execution_count":null,"id":"d6a58753-edec-4d28-a8cf-1de818d1a2a9","metadata":{"id":"d6a58753-edec-4d28-a8cf-1de818d1a2a9"},"outputs":[],"source":["# Determine the loss function based on the number of classes\n","if len(categories) \u003e 2:\n","    loss_function = \"categorical_crossentropy\"\n","else:\n","    loss_function = \"binary_crossentropy\"\n","\n","# Compile the model\n","opt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\n","model.compile(loss=loss_function, optimizer=opt, metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"id":"dcbcff1b-2868-4b53-8959-86f397fdc51f","metadata":{"id":"dcbcff1b-2868-4b53-8959-86f397fdc51f"},"outputs":[],"source":["# Train the model\n","H = model.fit(\n","    AugmentedData.flow(trainX, trainY, batch_size=BS),\n","    steps_per_epoch=len(trainX) // BS,\n","    validation_data=(testX, testY),\n","    validation_steps=len(testX) // BS,\n","    epochs=EPOCHS\n",")"]},{"cell_type":"code","execution_count":null,"id":"764094c1-e0bc-4373-964b-6def38c46ce0","metadata":{"id":"764094c1-e0bc-4373-964b-6def38c46ce0"},"outputs":[],"source":["# Save training history to JSON\n","hist_df = pd.DataFrame(H.history)\n","with open(f'{model_name.lower()}_history.json', mode='w') as f:\n","    hist_df.to_json(f)"]},{"cell_type":"code","execution_count":null,"id":"95b772ea-b608-4be9-a7a0-4c413bcf1097","metadata":{"id":"95b772ea-b608-4be9-a7a0-4c413bcf1097"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import itertools\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, \"{:.2f}\".format(cm[i, j]),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] \u003e thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","\n","# Evaluate the model\n","print(\"[INFO] evaluating network...\")\n","predIdxs = model.predict(testX, batch_size=BS)\n","predIdxs = np.argmax(predIdxs, axis=1)\n","print(classification_report(testY.argmax(axis=1), predIdxs))\n","\n","# Generate confusion matrix\n","confusion_mtx = confusion_matrix(testY.argmax(axis=1), predIdxs)\n","\n","# Plot the confusion matrix\n","plt.figure(figsize=(8, 8))\n","plot_confusion_matrix(confusion_mtx, classes=categories, normalize=True, title='Normalized Confusion Matrix')\n","plt.show()\n","\n","# Plot training history\n","N = EPOCHS\n","\n","plt.style.use(\"ggplot\")\n","sns.set_palette(\"husl\")\n","\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n","plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n","plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n","plt.title(\"Training Loss and Accuracy\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend(loc=\"lower left\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"104f9556-d88a-4438-8d67-7f7ab1739df1","metadata":{"id":"104f9556-d88a-4438-8d67-7f7ab1739df1"},"outputs":[],"source":["# Save the model in Keras format\n","model.save(f\"{model_name.lower()}_skin_classification_model.keras\")"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3.10 (tensorflow)","language":"python","name":"tensorflow-py3.10"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":5}